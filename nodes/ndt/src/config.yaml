seed: 1

num_workers: 0
batch_size: 512

seq_len: 30
input_dim: 142

n_heads: 1
n_layers: 4
hidden_size: 1024
undivided_attn: false
activation: relu

dropout_embedding: 0.1
dropout_mid_attn: 0.0
dropout_post_attn: 0.2
dropout_mid_mlp: 0.0
dropout_post_mlp: 0.1
dropout_rates: 0.0

pre_norm: true
mha_norm: layer
mlp_norm: layer
final_norm: scale

t_fixup: true
decoder_initrange: 0.001

context_forward: 30
context_backward: 5
